{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d36754",
   "metadata": {},
   "source": [
    "# ANS 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2dde6c",
   "metadata": {},
   "source": [
    " Features are the basic building blocks of datasets. The quality of the features in your dataset has a major impact on the quality of the insights you will gain when you use that dataset for machine learning.\n",
    "\n",
    "Additionally, different business problems within the same industry do not necessarily require the same features, which is why it is important to have a strong understanding of the business goals of your data science project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b80a61b",
   "metadata": {},
   "source": [
    "# ANS 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a17ef0",
   "metadata": {},
   "source": [
    "The features in your data will directly influence the predictive models you use and the results you can achieve. Your results are dependent on many inter-dependent properties. You need great features that describe the structures inherent in your data. Better features means flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b81fb72",
   "metadata": {},
   "source": [
    "# ANS 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63499841",
   "metadata": {},
   "source": [
    "Nominal data is made of discrete values with no numerical relationship between the different categories — mean and median are meaningless. Animal species is one example. For example, pig is not higher than bird and lower than fish."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10e9aa5",
   "metadata": {},
   "source": [
    "# ANS 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba93cf1",
   "metadata": {},
   "source": [
    " Converting categorical features into numeric features using domain knowledge. For example, we are given a list of countries and say we know the distance to these countries from India then we can replace it with distance from India. So, every country can be represented as its distance from India."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30429680",
   "metadata": {},
   "source": [
    "# ANS 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6bed1e",
   "metadata": {},
   "source": [
    "Wrapper methods measure the “usefulness” of features based on the classifier performance. In contrast, the filter methods pick up the intrinsic properties of the features (i.e., the “relevance” of the features) measured via univariate statistics instead of cross-validation performance.\n",
    "\n",
    "The wrapper classification algorithms with joint dimensionality reduction and classification can also be used but these methods have high computation cost, lower discriminative power. Moreover, these methods depend on the efficient selection of classifiers for obtaining high accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1106c0",
   "metadata": {},
   "source": [
    "# ANS 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001d1396",
   "metadata": {},
   "source": [
    "Features are considered relevant if they are either strongly or weakly relevant, and are considered irrelevant otherwise.\n",
    "\n",
    "Irrelevant features can never contribute to prediction accuracy, by definition. Also to quantify it we need to first check the list of features, There are three types of feature selection:\n",
    "\n",
    "- Wrapper methods (forward, backward, and stepwise selection)\n",
    "- Filter methods (ANOVA, Pearson correlation, variance thresholding)\n",
    "- Embedded methods (Lasso, Ridge, Decision Tree)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd65389f",
   "metadata": {},
   "source": [
    "# ANS 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ad0323",
   "metadata": {},
   "source": [
    "If two features {X1, X2} are highly correlated, then the two features become redundant features since they have same information in terms of correlation measure. In other words, the correlation measure provides statistical association between any given a pair of features.\n",
    "\n",
    "Minimum redundancy feature selection is an algorithm frequently used in a method to accurately identify characteristics of genes and phenotypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d21eb62",
   "metadata": {},
   "source": [
    "# ANS 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40d646c",
   "metadata": {},
   "source": [
    "Four of the most commonly used distance measures in machine learning are as follows:\n",
    "\n",
    "- Hamming Distance.\n",
    "- Euclidean Distance\n",
    "- Manhattan Distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3954cf0e",
   "metadata": {},
   "source": [
    "# ANS 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31f8f4d",
   "metadata": {},
   "source": [
    "Euclidean & Hamming distances are used to measure similarity or dissimilarity between two sequences. Euclidean distance is extensively applied in analysis of convolutional codes and Trellis codes.\n",
    "\n",
    "Hamming distance is frequently encountered in the analysis of block codes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c14c50e",
   "metadata": {},
   "source": [
    "# ANS 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1ea612",
   "metadata": {},
   "source": [
    "Feature selection is for filtering irrelevant or redundant features from your dataset. The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d0934e",
   "metadata": {},
   "source": [
    "# ANS 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fad555",
   "metadata": {},
   "source": [
    "- The width of the silhouette\n",
    "\n",
    "The Average Silhouette Width (ASW) is a popular cluster validation index to estimate the number of clusters. The question whether it also is suitable as a general objective function to be optimized for finding a clustering is addressed. Two algorithms (the standard version OSil and a fast version FOSil) are proposed, and they are compared with existing clustering methods in an extensive simulation study covering known and unknown numbers of clusters. Real data sets are analysed, partly exploring the use of the new methods with non-Euclidean distances. The ASW is shown to satisfy some axioms that have been proposed for cluster quality functions. The new methods prove useful and sensible in many cases, but some weaknesses are also highlighted. These also concern the use of the ASW for estimating the number of clusters together with other methods, which is of general interest due to the popularity of the ASW for this task.\n",
    "\n",
    "- Receiver operating characteristic curve\n",
    "\n",
    "A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.\n",
    "The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The true-positive rate is also known as sensitivity, recall or probability of detection.The false-positive rate is also known as probability of false alarm and can be calculated as (1 − specificity). It can also be thought of as a plot of the power as a function of the Type I Error of the decision rule (when the performance is calculated from just a sample of the population, it can be thought of as estimators of these quantities). The ROC curve is thus the sensitivity or recall as a function of fall-out. In general, if the probability distributions for both detection and false alarm are known, the ROC curve can be generated by plotting the cumulative distribution function (area under the probability distribution from {\\displaystyle -\\infty }-\\infty  to the discrimination threshold) of the detection probability in the y-axis versus the cumulative distribution function of the false-alarm probability on the x-axis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
