{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42f153a7",
   "metadata": {},
   "source": [
    "# ANS 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f05280",
   "metadata": {},
   "source": [
    "Feature engineering is the process of selecting, manipulating, and transforming raw data into features that can be used in supervised learning. In order to make machine learning work well on new tasks, it might be necessary to design and train better features.\n",
    "\n",
    "Feature engineering in ML consists of four main steps: Feature Creation, Transformations, Feature Extraction, and Feature Selection. Feature engineering consists of creation, transformation, extraction, and selection of features, also known as variables, that are most conducive to creating an accurate ML algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d39829",
   "metadata": {},
   "source": [
    "# ANS 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14299cd1",
   "metadata": {},
   "source": [
    " Feature Selection is the process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are interested in. Having irrelevant features in your data can decrease the accuracy of the models and make your model learn based on irrelevant features.\n",
    "\n",
    "There are three types of feature selection:\n",
    "\n",
    "- Wrapper methods (forward, backward, and stepwise selection)\n",
    "- Filter methods (ANOVA, Pearson correlation, variance thresholding)\n",
    "- Embedded methods (Lasso, Ridge, Decision Tree)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201c3b78",
   "metadata": {},
   "source": [
    "# ANS 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde8bb0b",
   "metadata": {},
   "source": [
    " The main differences between the filter and wrapper methods for feature selection are: Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a model on it.\n",
    "\n",
    "The filter method has the fastest running time; however, it does not consider feature dependencies and tends to each feature separately when univariate techniques are used. The wrapper method has the advantages of better generalization and robust interaction with the classifier used for feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f77f323",
   "metadata": {},
   "source": [
    "# ANS 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a88fbb",
   "metadata": {},
   "source": [
    "i. Feature selection is the process of reducing the number of input variables when developing a predictive model. It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model.\n",
    "\n",
    "ii. Feature Extraction aims to reduce the number of features in a dataset by creating new features from the existing ones (and then discarding the original features). These new reduced set of features should then be able to summarize most of the information contained in the original set of features. In this way, a summarised version of the original features can be created from a combination of the original set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a1cc34",
   "metadata": {},
   "source": [
    "# ANS 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4057780",
   "metadata": {},
   "source": [
    "Text classification is the problem of assigning categories to text data according to its content. The most important part of text classification is feature engineering: the process of creating features for a machine learning model from raw text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899e6bb3",
   "metadata": {},
   "source": [
    "# ANS 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556067ff",
   "metadata": {},
   "source": [
    "Cosine similarity is a metric used to measure how similar the documents are irrespective of their size. The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together.\n",
    "\n",
    "Cosine similarity is the cosine of the angle between two n-dimensional vectors in an n-dimensional space. It is the dot product of the two vectors divided by the product of the two vectors' lengths (or magnitudes).\n",
    "\n",
    "The ‘x’ vector has values, x = { 2, 3, 2, 0, 2, 3, 3, 0, 1 }\n",
    "The ‘y’ vector has values, y = { 2, 1, 0, 0, 3, 2, 1, 3, 1 }\n",
    "The formula for calculating the cosine similarity is : Cos(x, y) = x . y / ||x|| * ||y||\n",
    "\n",
    "x . y = 2*2 + 3*1 + 2*0 + 0*0 + 2*3 + 3*2 + 3*1 + 0*3 + 1*1 = 23\n",
    "\n",
    "||x|| = √ (2)^2 + (3)^2 + (2)^2 + (0)^2 + (2)^2 + (3)^2 + (3)^2 + (0)^2 + (1)^2 = 6.32\n",
    "\n",
    "||y|| = √ (2)^2 + (1)^2 + (0)^2 + (0)^2 + (3)^2 + (2)^2 + (1)^2 + (3)^2 + (1)^2 = 5.38\n",
    "\n",
    "∴ Cos(x, y) = 23 / (6.32 * 5.38) = 0.67"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d23645",
   "metadata": {},
   "source": [
    "# ANS 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46be27c7",
   "metadata": {},
   "source": [
    " To calculate the Hamming distance, you simply count the number of bits where two same-length messages differ. An example of Hamming distance 1 is the distance between 1101 and 1001 . If you increase the distance to 2 , we can give as an example 1001 and 1010 \n",
    "\n",
    "Hamming gap between 10001011 and 11001111 is 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5629d1",
   "metadata": {},
   "source": [
    "# ANS 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ef9bae",
   "metadata": {},
   "source": [
    "High dimension is when variable numbers p is higher than the sample sizes n i.e. p>n, cases. High dimensional data is referred to a data of n samples with p features, where p is larger than n.\n",
    "\n",
    "For example, tomographic imaging data, ECG data, and MEG data. One example of high dimensional data is microarray gene expression data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f36ce9",
   "metadata": {},
   "source": [
    "# ANS 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6315a2",
   "metadata": {},
   "source": [
    "The Principal component analysis (PCA) is a technique used for identification of a smaller number of uncorrelated variables known as principal components from a larger set of data. The technique is widely used to emphasize variation and capture strong patterns in a data set.\n",
    "\n",
    "Vectors can be used to represent physical quantities. Most commonly in physics, vectors are used to represent displacement, velocity, and acceleration. Vectors are a combination of magnitude and direction, and are drawn as arrows\n",
    "\n",
    "In the context of machine learning, an embedding is a low-dimensional, learned continuous vector representation of discrete variables into which you can translate high-dimensional vectors. Generally, embeddings make ML models more efficient and easier to work with, and can be used with other models as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab731184",
   "metadata": {},
   "source": [
    "# ANS 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22cc3b8",
   "metadata": {},
   "source": [
    "1. In the forward method, the software looks at all the predictor variables you selected and picks the one that predicts the most on the dependent measure. That variable is added to the model. This is repeated with the variable that then predicts the most on the dependent measure. This little procedure continues until adding predictors does not add anything to the prediction model anymore.\n",
    "In the backward method, all the predictor variables you chose are added into the model. Then, the variables that do not (significantly) predict anything on the dependent measure are removed from the model one by one.\n",
    "The backward method is generally the preferred method, because the forward method produces so-called suppressor effects. These suppressor effects occur when predictors are only significant when another predictor is held constant.\n",
    "\n",
    "\n",
    "2.  Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a model on it.\n",
    "\n",
    "\n",
    "3. Sequential floating forward selection (SFFS) starts from the empty set. After each forward step, SFFS performs backward steps as long as the objective function increases. Sequential floating backward selection (SFBS) starts from the full set.\n",
    "The Jaccard coefficient is a measure of the percentage of overlap between sets defined as: (5.1) where W1 and W2 are two sets, in our case the 1-year windows of the ego networks. The Jaccard coefficient can be a value between 0 and 1, with 0 indicating no overlap and 1 complete overlap between the sets.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
