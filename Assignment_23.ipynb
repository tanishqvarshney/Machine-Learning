{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d42048c6",
   "metadata": {},
   "source": [
    "# ANS 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872f51b7",
   "metadata": {},
   "source": [
    "Disadvantages of Dimensionality Reduction:\n",
    "\n",
    "- It may lead to some amount of data loss.\n",
    "- PCA tends to find linear correlations between variables, which is sometimes undesirable.\n",
    "- PCA fails in cases where mean and covariance are not enough to define datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b034c7f",
   "metadata": {},
   "source": [
    "# ANS 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307dce79",
   "metadata": {},
   "source": [
    "The curse of dimensionality basically means that the error increases with the increase in the number of features. It refers to the fact that algorithms are harder to design in high dimensions and often have a running time exponential in the dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71f9038",
   "metadata": {},
   "source": [
    "# ANS 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abb6b6d",
   "metadata": {},
   "source": [
    "No, dimensionality reduction is not reversible in general."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42c1a1f",
   "metadata": {},
   "source": [
    "# ANS 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c2ef59",
   "metadata": {},
   "source": [
    "Well it Depends on dataset. If it is comprised of points that are perfectly aligned, PCA can reduce the dataset down to 1 dimension and preserve 95% of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25701d79",
   "metadata": {},
   "source": [
    "# ANS 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1881778e",
   "metadata": {},
   "source": [
    "If I perform PCA on a 1,000-dimensional dataset, setting the explained variance ratio to 95%. In this case roughly 950 dimensions are required to preserve 95% of the variance. So the answer is, it depends on the dataset, and it could be any number between 1 and 950."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76b3780",
   "metadata": {},
   "source": [
    "# ANS 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b766a3b",
   "metadata": {},
   "source": [
    "The following are the scenarios where the following are used:\n",
    "\n",
    "- Vanilla PCA: the dataset fit in memory\n",
    "- Incremental PCA: larget dataset that don't fit in memory, online taks\n",
    "- Randomized PCA: considerably reduce dimensionality and the dataset fit the memory.\n",
    "- kernel PCA: used for nonlinear PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b37d23",
   "metadata": {},
   "source": [
    "# ANS 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b619026",
   "metadata": {},
   "source": [
    "By doing PCA, it is a good choice for dimensionality reduction and visualization for datasets with a large number of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c4f3e2",
   "metadata": {},
   "source": [
    "# ANS 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470ddb83",
   "metadata": {},
   "source": [
    " Indeed, it often make any sense to chain two different dimensionality reduction algorithms. A common example is using PCA to quickly get rid of a large number of useless dimensions, then applying another much slower dimensionality reduction algorithm, such as LLE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
